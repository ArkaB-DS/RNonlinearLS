% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Refactoring the nls() function in R},
  pdfauthor={John C. Nash ; Arkajyoti Bhattacharjee },
  colorlinks=true,
  linkcolor={red},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Refactoring the \texttt{nls()} function in R}
\author{John C. Nash
\thanks{ retired professor, Telfer School of Management, University of Ottawa} \and Arkajyoti
Bhattacharjee
\thanks{Department of Mathematics and Statistics, Indian Institute of Technology, Kanpur}}
\date{2022-6-15}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

Based on work for a Google Summer of Code project ``Improvements to
\texttt{nls()}'', we consider the features and limitations of the R
function \texttt{nls()} in the context of improving and rationalizing R
tools for nonlinear regression.

Important considerations are the usability and maintainability of the
code base that provides the functionality \texttt{nls()} claims to
offer. Our work suggests that the existing code makes maintenance and
improvement very difficult, with legacy applications and examples
blocking some important updates. Discussion of these matters is relevant
to improving R generally, as well as its nonlinear estimation tools.

\hypertarget{the-nls-function-strengths-and-shortcomings}{%
\section{\texorpdfstring{The \texttt{nls()} function: strengths and
shortcomings}{The nls() function: strengths and shortcomings}}\label{the-nls-function-strengths-and-shortcomings}}

\texttt{nls()} is the tool in base R, the primary software software
distribution from the Comprehensive R Archive Network
(\url{https://cran.r-project.org}) for estimating nonlinear statistical
models. The function dates to the 1980s and the work related to D. M.
Bates and Watts (\protect\hyperlink{ref-bateswatts}{1988}) in S (see
\url{https://en.wikipedia.org/wiki/S_\%28programming_language\%29}).

The \texttt{nls()} function has a remarkable and comprehensive set of
capabilities for estimating nonlinear models that are expressed as
\textbf{formulas}. In particular, we note that it

\begin{itemize}
\tightlist
\item
  handles formulas that include R functions, even ones which call
  calculations in other programming languages
\item
  allows data to be weighted or subset
\item
  can estimate bound constrained parameters
\item
  provides a mechanism for handling partially linear models
\item
  permits parameters to be indexed over a set of related data
\item
  produces measures of variability (i.e., standard error estimates) for
  the estimated parameters
\item
  has related profiling capabilities for exploring the likelihood
  surface as parameters are changed
\item
  links to a number of pre-coded (``selfStart'') models
\end{itemize}

With such a range of features and long history, the code has become
untidy and overly patched. It is, to our mind, essentially
unmaintainable. Moreover, its underlying methods can and should be
improved, as we suggest below.

\hypertarget{feature-convergence-and-termination-tests-fixed}{%
\subsection{Feature: Convergence and termination tests
(FIXED)}\label{feature-convergence-and-termination-tests-fixed}}

A previous issue with \texttt{nls()} that prevented it from providing
parameter estimates for zero-residual (i.e., perfect fit) data was
corrected thanks to suggestions by one of us.

In the manual page for \texttt{nls()} in R 4.0.0 there is the warning:

\begin{quote}
\textbf{Do not use \texttt{nls} on artificial ``zero-residual'' data.}
\end{quote}

\begin{quote}
The \texttt{nls} function uses a relative-offset convergence criterion
that compares the numerical imprecision at the current parameter
estimates to the residual sum-of-squares. This performs well on data of
the form
\end{quote}

\begin{quote}
\[  y = f(x, \theta) + eps \]
\end{quote}

\begin{quote}
(with \texttt{var(eps)\ \textgreater{}\ 0}). It fails to indicate
convergence on data of the form
\end{quote}

\begin{quote}
\[  y = f(x, \theta)  \]
\end{quote}

\begin{quote}
because the criterion amounts to comparing two components of the
round-off error.
\end{quote}

\begin{quote}
If you wish to test \texttt{nls} on artificial data please add a noise
component, as shown in the example below.
\end{quote}

This amounted to admitting R cannot solve well-posed problems unless
data is polluted with errors.

This issue can be easily resolved. The ``termination test'' for the
\textbf{program} rather than for ``convergence'' of the underlying
\textbf{algorithm} is the Relative Offset Convergence Criterion (see
Douglas M. Bates and Watts
(\protect\hyperlink{ref-BatesWatts81}{1981})). This projects the
proposed step in the parameter vector on the gradient and estimates how
much the sum of squares loss function should decrease. This estimate is
divided by the current size of the loss function to avoid scale issues.
When we have ``converged'', the estimated decrease is very small.
However, with small residuals, the sum of squares loss function is
(almost) zero and we get the possibility of a zero-divide failure.

Adding a small quantity to the loss function before dividing avoids
trouble. In 2021, one of us (J. Nash) proposed that
\texttt{nls.control()} have an additional parameter \texttt{scaleOffset}
with a default value of zero. Setting it to a small number -- 1.0 is a
reasonable choice -- allows small-residual problems (i.e., near-exact
fits) to be dealt with easily. We call this the \textbf{safeguarded
relative offset convergence criterion}. The default value gives the
legacy behaviour. We are pleased to note that this improvement is now in
the R distributed code as of version 4.1.0.

\hypertarget{more-general-termination-tests}{%
\subsubsection{More general termination
tests}\label{more-general-termination-tests}}

The convergence criterion above, the principal termination control of
\texttt{nls()}, leaves out some possibilities that could be useful for
some problems. The package \texttt{nlsr} (John C. Nash and Murdoch
(\protect\hyperlink{ref-nlsr-manual}{2019})) already offers both the
safeguarded relative offset test (\textbf{roffset}) as well as a
\textbf{small sum of squares} test (\textbf{smallsstest}) that compares
the latest evaluated sum of squared (weighted) residuals to \texttt{e4}
times the initial sum of squares, where
\texttt{e4\ \textless{}-\ (100*.Machine\$double.eps)\^{}4} is
approximately 2.43e-55.

We note that \texttt{nls()} uses a termination test to stop after
\texttt{maxiter} ``iterations''. Unfortunately, the meaning of
``iteration'' varies among programs and requires careful examination of
the code. We prefer to use the number of times the residuals or the
jacobian have been computed and put upper limits on these. Our codes
exit (terminate) when these limits are reached. Generally we prefer
larger limits than the default \texttt{maxiter\ =\ 50} of
\texttt{nls()}, but that may reflect the more difficult problems we have
encountered because users consult us when standard tools have given
unsatisfactory results.

\hypertarget{feature-failure-when-jacobian-is-computationally-singular}{%
\subsection{Feature: Failure when Jacobian is computationally
singular}\label{feature-failure-when-jacobian-is-computationally-singular}}

This refers to the infamous ``singular gradient'' termination message of
\texttt{nls()}. A Google search of

\begin{verbatim}
R nls "singular gradient"
\end{verbatim}

gets over 4000 hits that are spread over some years. This could be
because the Jacobian is poorly approximated (see \textbf{Jacobian
computation} below). However, it is common in nonlinear least squares
computations that the Jacobian is very close to singular for some values
of the model parameters. In such cases, we need to find an alternative
algorithm to the Gauss-Newton iteration of \texttt{nls()}. The most
common work-around is the Levenberg-Marquardt stabilization (see
Marquardt (\protect\hyperlink{ref-Marquardt1963}{1963}), Levenberg
(\protect\hyperlink{ref-Levenberg1944}{1944}), John C. Nash
(\protect\hyperlink{ref-jn77ima}{1977})), and versions of it have been
implemented in packages \texttt{minpack.lm} and \texttt{nlsr}. In our
work, we prepared experimental prototypes of \texttt{nls} that
incorporate stabilization, but integration with all the features of
\texttt{nls()} has proved difficult.

Let us consider a `singular gradient' example
\emph{\url{https://stats.stackexchange.com/questions/13053/singular-gradient-error-in-nls-with-correct-starting-values}}.
\texttt{nlsr::nlxb()} displays the singular values of the Jacobian,
which confirm that the \texttt{nls()} error message is correct, but this
is not helpful in obtaining a solution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reala }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{3}\NormalTok{; realb }\OtherTok{\textless{}{-}} \DecValTok{5}\NormalTok{; realc }\OtherTok{\textless{}{-}} \FloatTok{0.5}\NormalTok{; realr }\OtherTok{\textless{}{-}} \FloatTok{0.7}\NormalTok{; realm }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\# underlying parameters}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{11} \CommentTok{\# x values; 11 data points}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ reala }\SpecialCharTok{+}\NormalTok{ realb }\SpecialCharTok{*}\NormalTok{ realr}\SpecialCharTok{\^{}}\NormalTok{(x }\SpecialCharTok{{-}}\NormalTok{ realm) }\SpecialCharTok{+}\NormalTok{ realc }\SpecialCharTok{*}\NormalTok{ x }\CommentTok{\# linear + exponential function}
\NormalTok{testdat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(x, y) }\CommentTok{\# save in a data frame}
\NormalTok{strt }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{a =} \SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\AttributeTok{b =} \DecValTok{5}\NormalTok{, }\AttributeTok{c =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{r =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{) }\CommentTok{\# give programs a good start}
\NormalTok{jform }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ b }\SpecialCharTok{*}\NormalTok{ r}\SpecialCharTok{\^{}}\NormalTok{(x }\SpecialCharTok{{-}}\NormalTok{ m) }\SpecialCharTok{+}\NormalTok{ c }\SpecialCharTok{*}\NormalTok{ x }\CommentTok{\# Formula}
\FunctionTok{library}\NormalTok{(nlsr)}
\NormalTok{linexp2 }\OtherTok{\textless{}{-}} \FunctionTok{try}\NormalTok{(}\FunctionTok{nlxb}\NormalTok{(jform, }\AttributeTok{data =}\NormalTok{ testdat, }\AttributeTok{start =}\NormalTok{ strt, }\AttributeTok{trace =}\NormalTok{ F))}
\NormalTok{linexp2 }\CommentTok{\# Note singular values of Jacobian in rightmost column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## nlsr object: x 
## residual sumsquares =  0  on  11 observations
##     after  1    Jacobian and  1 function evaluations
##   name            coeff          SE       tstat      pval      gradient    JSingval   
## a                     -3            NA         NA         NA           0       26.49  
## b                      5            NA         NA         NA           0       9.615  
## c                    0.5            NA         NA         NA           0       2.466  
## r                    0.7            NA         NA         NA           0      0.1098  
## m                      1            NA         NA         NA           0   1.398e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We get an error with nls()}
\NormalTok{linexp }\OtherTok{\textless{}{-}} \FunctionTok{try}\NormalTok{(}\FunctionTok{nls}\NormalTok{(jform, }\AttributeTok{data =}\NormalTok{ testdat, }\AttributeTok{start =}\NormalTok{ strt, }\AttributeTok{trace =}\NormalTok{ F))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in nlsModel(formula, mf, start, wts, scaleOffset = scOff, nDcentral = nDcntr) : 
##   singular gradient matrix at initial parameter estimates
\end{verbatim}

The Jacobian is essentially singular as shown by its singular values.
Note that these are \textbf{displayed} by package \texttt{nlsr} in a
single column in the output to provide a compact layout, but the values
do \textbf{NOT} correspond to the individual parameters in whose row
they appear; they are a property of the whole problem.

\hypertarget{feature-jacobian-computation}{%
\subsection{Feature: Jacobian
computation}\label{feature-jacobian-computation}}

\texttt{nls()}, with the \texttt{numericDeriv()} function, computes the
Jacobian as the ``gradient'' attribute of the residual vector. This is
implemented as a mix of R and C code, but we have created a rather more
compact version entirely in R. See
\url{https://github.com/nashjc/RNonlinearLS/DerivsNLS} .

As far as we can understand the logic in \texttt{nls()}, the Jacobian
computation during parameter estimation is carried out entirely within
the called C-language program, and the R code
\texttt{function\ numericDeriv()}, part of
\texttt{./src/library/stats/R/nls.R} in the R distribution source code,
is used to provide Jacobian information in the \texttt{nlsModel()} and
\texttt{nlsModel.plinear()} functions.

The Jacobian used by \texttt{nlsr::nlxb()} is, by default, computed from
analytic expressions, while that of \texttt{nls()} mostly uses a
numerical approximation. Some selfStart models do provide ``gradient''
information. \texttt{minpack.lm::nlsLM()} invokes
\texttt{numericDeriv()} in its local version of \texttt{nlsModel()}, but
it appears to use an internal approximate Jacobian code from the
original Fortran \texttt{minpack} code, namely, \texttt{lmdif.f}. Such
differences in approach can lead to different behaviour, though in our
experience, one has to look for the differences, as they are generally
minor.

\begin{itemize}
\item
  A pasture regrowth problem (Huet et al.
  (\protect\hyperlink{ref-Huet2004}{2004}), page 1, based on Ratkowsky
  (\protect\hyperlink{ref-Ratkowsky1983}{1983})) has a poorly
  conditioned Jacobian and \texttt{nls()} fails with ``singular
  gradient''. Worse, numerical approximation to the Jacobian give the
  error ``singular gradient matrix at initial parameter estimates'' for
  \texttt{minpack.lm::nlsLM} so that the Marquardt stabilization is
  unable to take effect, while the analytic derivatives of
  \texttt{nlsr::nlxb} give a solution.
\item
  Karl Schilling (private communication) provided an example where a
  model specified with the formula
  \texttt{y\ \textasciitilde{}\ a\ *\ (x\ ˆ\ b)} causes
  \texttt{nlsr::nlxb} to fail because the partial derivative w.r.t.
  \texttt{b} is \texttt{a\ *\ (x\^{}b\ *\ log(x))}. If there is data for
  which \texttt{x\ =\ 0}, this is undefined. In such cases, we observed
  that \texttt{nls()} and \texttt{minpack.lm::nlsLM} found a solution,
  though it can be debated whether such lucky accidents can be taken as
  an advantage.
\end{itemize}

Note that the selfStart models in the base R
\texttt{./src/library/stats/R/zzModels.R} file may provide the Jacobian
in the ``gradient'' attribute of the ``one-sided'' formula that defines
each model, and these Jacobians may be the analytic forms. The
\texttt{nls()} function, after computing the ``right hand side'' or
\texttt{rhs} of the residual, then checks to see if the ``gradient''
attribute is defined, and, if not, uses \texttt{numericDeriv} to compute
a Jacobian into that attribute. This code is within the
\texttt{nlsModel()} or \texttt{nlsModel.plinear()} functions. The use of
analytic Jacobians almost certainly contributes to the good performance
of \texttt{nls()} on selfStart models.

\hypertarget{feature-weights-on-observations}{%
\subsection{Feature: Weights on
observations}\label{feature-weights-on-observations}}

?? does resid return weighted or unweighted residuals

?? do we want unweighted (raw) residuals? or are predictions / fits just
as good?

\hypertarget{feature-subsetting}{%
\subsection{Feature: Subsetting}\label{feature-subsetting}}

\texttt{nls()} accepts an argument \texttt{subset}. This acts through
the mediation of \texttt{model.frame} and is not clearly obvious in the
source code files \texttt{/src/library/stats/R/nls.R} and
\texttt{/src/library/stats/src/nls.C}.

While the implementation of subset at the level of the call to
\texttt{nls()} has a certain attractiveness, it does mean that the
programmer of the solver needs to be aware of the source (and value) of
objects such as the data, residuals and Jacobian. By preference, we
would implement subsetting by means of zero-value weights, with
observation counts (and degrees of freedom) computed via the numbers of
non-zero weights. Alternatively, we would extract a working dataframe
from the relevant elements in the original.

\hypertarget{feature-na.action}{%
\subsection{Feature: na.action}\label{feature-na.action}}

\texttt{na.action} is an argument to the \texttt{nls()} function, but it
does not appear obviously in the source code, often being handled behind
the scenes after referencing the option \texttt{na.action}. A useful,
but possibly dated, description is given in:
\url{https://stats.idre.ucla.edu/r/faq/how-does-r-handle-missing-values/}.

The typical default action, which can be seen by using the command
\texttt{getOption("na.action")} is \texttt{na.omit}. This option
essentially omits from computations any observations containing missing
values (i.e.~any row of a data frame containing an NA).
\texttt{na.exclude} does much of the same for computations, but keeps
the rows with NA elements so that predictions are in the correct row
position. We recommend that workers actually test output to verify the
behaviour is as wanted.

A succinct description of the issue is given in:
\emph{\url{https://stats.stackexchange.com/questions/492955/should-i-use-na-omit-or-na-exclude-in-a-linear-model-in-r}}

\begin{quote}
The only benefit of \texttt{na.exclude} over \texttt{na.omit} is that
the former will retain the original number of rows in the data. This may
be useful where you need to retain the original size of the dataset -
for example it is useful when you want to compare predicted values to
original values. With \texttt{na.omit} you will end up with fewer rows
so you won't as easily be able to compare.
\end{quote}

\texttt{na.pass} simply passes on data ``as is'', while \texttt{na.fail}
will essentially stop if any missing values are present.

Our concern with \texttt{na.action} is that users may be unaware of the
effects of an option setting they may not even be aware has been set. Is
\texttt{na.fail} a safer default?

\hypertarget{feature-model-frame}{%
\subsection{Feature: model frame}\label{feature-model-frame}}

\texttt{model} is an argument to the \texttt{nls()} function, which is
documented as:

\begin{quote}
\textbf{model} logical. If true, the model frame is returned as part of
the object. Default is FALSE.
\end{quote}

Indeed, the argument only gets used when \texttt{nls()} is about to
return its result object, and the element \texttt{model} is NULL unless
the calling argument \texttt{model} is TRUE. (Using the same name could
be confusing.) However, the model frame is used within the function code
in the form of the object \texttt{mf}. We feel that users could benefit
from more extensive documentation and examples of its use.

\hypertarget{feature-sources-of-data}{%
\subsection{Feature: Sources of data}\label{feature-sources-of-data}}

\texttt{nls()} can be called without specifying the \texttt{data}
argument. In this case, it will search in the available environments
(i.e., workspaces) for suitable data objects. We do NOT like this
approach, but it is ``the R way''. R allows users to leave many objects
in the default (.GlobalEnv) workspace. Moreover, users have to actively
suppress saving this workspace (\texttt{.RData}) on exit; otherwise, any
such file in the path, when R is launched, will be loaded. The
overwhelming proportion of R users in our acquaintance avoid saving the
workspace because of the danger of lurking data and functions which may
cause unwanted results.

Nevertheless, to provide compatible behaviour with \texttt{nls()},
competing programs need to ensure equivalent behaviour, but users should
test that the operation is as they intend.

\hypertarget{feature-missing-start-vector-and-self-starting-models}{%
\subsection{Feature: missing start vector and self-starting
models}\label{feature-missing-start-vector-and-self-starting-models}}

Nonlinear estimation algorithms are almost all iterative and need a set
of starting parameters. \texttt{nls()} offers a special class of
modeling functions called \textbf{selfStart} models. There are a number
of these in base R (\texttt{./src/library/stats/R/zzModels.R}) and
others in R packages such as CRAN package \texttt{nlraa} (Miguez
(\protect\hyperlink{ref-MiguezNLRAA2021}{2021})), as well as the
now-archived package \texttt{NRAIA}. Unfortunately, some
\textbf{selfStart} codes entangle the calculation of starting values for
parameters with the particulars of the \texttt{nls()} code. Though there
is a \texttt{getInitial()} function, this is not easy to use to simply
compute the initial parameter estimates outside of \texttt{nls()}, in
part because it may call that function. Such circular references are, in
our view, dangerous. Moreover, we believe that it would be helpful to
have selfStart models that allow users to explicitly provide the
starting parameters.

Other concerns are revealed by the example below. Here, the
\texttt{SSlogis} selfStart function is used to generate a set of initial
parameters for a 3-parameter logistic curve. The form used by
\texttt{SSlogis} is
\(y \,\sim\, Asym\,/\, (1 \,+\, exp((xmid\,-\,tt)\,/\,scal))\), but we
show how the starting parameters for this model can be transformed to
those of another form of the model, namely,
\(y \,\sim\, b1\,/\,(1 \,+\, b2\,*\,exp(-b3\,*\,t))\).

The code for \texttt{SSlogis()} is in
\texttt{./src/library/stats/R/zzModels.R}. This R function includes
analytic expressions for the Jacobian (``gradient''). These could be
useful to R users, especially if documented. Moreover, we wonder why the
programmers have chosen to save so many quantities in ``hidden''
variables, i.e., with names preceded by ``.''. These are then not
displayed by the \texttt{ls()} command, making them difficult to access.

In the event that a selfStart model is not available, \texttt{nls()}
sets all the starting parameters to 1. This is, in our view, tolerable,
but could be improved by using a set of values that are all slightly
different, which, in the case of the example model
\(y \,\sim\, a \,*\, exp(-b \,*\, x) + c\,*\,exp(-d \,*\, x)\) would
avoid a singular Jacobian because \(b\) and \(d\) were equal in value. A
sequence like 1.0, 1.1, 1.2, 1.3 for the four parameters could be
provided quite simply instead of all 1's.

\hypertarget{strategic-issues-in-selfstart-models}{%
\subsubsection{Strategic issues in selfStart
models}\label{strategic-issues-in-selfstart-models}}

Because the Gauss-Newton algorithm is unreliable from many sets of
starting of parameters, selfStart models are a part of the
\texttt{nls()} infrastructure rather than an accessory. However,
creating such functions is a lot of work, and their documentation (file
\texttt{./src/library/stats/man/selfStart.Rd}) is quite complicated. We
believe that the focus should be placed on getting good initial
parameters, that is \texttt{getInitial()} function, though avoiding the
current calls back to \texttt{nls()}. Interactive tools, such as
``visual fitting'' (John C. Nash and Velleman
(\protect\hyperlink{ref-nash1996nonlinear}{1996})) might be worth
considering.

We also note that the introduction of \texttt{scaleOffset} in R 4.1.1 to
deal with the convergence test for small residual problems now requires
that the \texttt{getInitial()} function have dot-arguments
(\texttt{...}) in its argument list. This illustrates the entanglement
of many features in \texttt{nls()}.

\hypertarget{issue-returned-output-of-nls-and-its-documentation}{%
\subsection{Issue: returned output of nls() and its
documentation}\label{issue-returned-output-of-nls-and-its-documentation}}

The output of \texttt{nls()} is an object of class ``nls'' which has the
following structure:

\begin{quote}
A list of
\end{quote}

\begin{quote}
\texttt{m} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~~ an \texttt{nlsModel} object
incorporating the model.
\end{quote}

\begin{quote}
\texttt{data} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~the expression that was
passed to \texttt{nls} as the data argument. The actual \newline
\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\) data values are present in the
environment of the \texttt{m} components, e.g.,
\newline \(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\)
\texttt{environment(m\$conv)}.
\end{quote}

\begin{quote}
\texttt{call} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ the matched call with several
components, notably \texttt{algorithm}.
\end{quote}

\begin{quote}
\texttt{na.action} ~ ~ ~ ~ ~ ~ ~ ~ ~ the \texttt{"na.action"} attribute
(if any) of the model frame.
\end{quote}

\begin{quote}
\texttt{dataClasses} ~ ~ ~ ~ ~ ~ ~ \(~\)the \texttt{"dataClasses"}
attribute (if any) of the ``\texttt{terms}'' attribute of the \newline
\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\) model frame.
\end{quote}

\begin{quote}
\texttt{model} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~if \texttt{model\ =\ TRUE}, the
model frame.
\end{quote}

\begin{quote}
\texttt{weights} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ if \texttt{weights} is supplied,
the weights.
\end{quote}

\begin{quote}
\texttt{convInfo} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~~ a list with convergence
information.
\end{quote}

\begin{quote}
\texttt{control} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ the control \texttt{list} used,
see the \texttt{control} argument.
\end{quote}

\begin{quote}
\texttt{convergence,\ message} ~for an \texttt{algorithm\ =\ "port"} fit
only, a convergence code (\texttt{0} for \newline
\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\)convergence) and message.
\end{quote}

\begin{quote}
~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ To use these is \emph{deprecated}, as
they are available from \texttt{convInfo} now.
\end{quote}

\begin{verbatim}
              -----------------------------------------------------------
\end{verbatim}

The \texttt{nls} object contains some elements that are awkward to
produce by other algorithms, but some information that would be useful
is not presented in a clear manner. Moreover, the complexity of the
object is a challenge to users.

In the following, we use \texttt{result} as the returned object from
\texttt{nls()}.

The \texttt{data} return element is an R symbol. To actually access the
data from this element, we need to use the syntax:

\begin{verbatim}
eval(parse(text=result$data))
\end{verbatim}

However, if the call is made with \texttt{model=TRUE}, then there is a
returned element \texttt{model} which contains the data, and we can list
its contents using:

\begin{verbatim}
ls(result$model)
\end{verbatim}

and if there is an element called \texttt{xdata}, it can be accessed as
\texttt{result\$model\$xdata}.

Let us compare the \texttt{nls()} result with that from
\texttt{nlsr::nlxb()}, with ostensibly solves the same problem:

\begin{quote}
\texttt{coefficients} A named vector giving the parameter values at the
supposed solution.
\end{quote}

\begin{quote}
\texttt{ssquares} \(~~~~~\) The sum of squared residuals at this set of
parameters.
\end{quote}

\begin{quote}
\texttt{resid} \(~~~~~~~~~~\) The residual vector at the returned
parameters.
\end{quote}

\begin{quote}
\texttt{jacobian} \(~~~~~\) The jacobian matrix (partial derivatives of
residuals w.r.t. the parameters) at the \newline
\(~~~~~~~~~~~~~~~~~~~~\) returned parameters.
\end{quote}

\begin{quote}
\texttt{feval} \(~~~~~~~~~~\) The number of residual evaluations (sum of
squares computations) used.
\end{quote}

\begin{quote}
\texttt{jeval} \(~~~~~~~~~~\) The number of Jacobian evaluations used.
\end{quote}

However, actually looking at the structure of a returned result gives a
list of 11 items. The extra 5 are:

\begin{verbatim}
 $ lower       : num [1:3] -Inf -Inf -Inf
 $ upper       : num [1:3] Inf Inf Inf
 $ maskidx     : int(0) 
 $ weights     : NULL
 $ formula     :Class 'formula'  language y ~ Asym/(1 + exp((xmid - tt)/scal))
  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
 - attr(*, "class")= chr "nlsr"
\end{verbatim}

The result object from \texttt{nlsr::nlxb()} is still much smaller than
the one \texttt{nls()} returns. Moreover, \texttt{nlxb} explicitly
returns the sum of squares as well as the residual vector and Jacobian.
The counts of evaluations are also returned. (Note that the singular
values of the Jacobian are actually computed in the \texttt{print} and
\texttt{summary} methods for the result.) We noted several potential
updates to the \texttt{nlsr} documentation as well as that for
\texttt{nls()} as we made our comparisons.

\hypertarget{weights-in-returned-functions-from-nls}{%
\subsubsection{Weights in returned functions from
nls()}\label{weights-in-returned-functions-from-nls}}

The functions \texttt{resid()} (an alias for \texttt{residuals()}) and
\texttt{fitted()} and \texttt{lhs()} are UNWEIGHTED. But if we return
\texttt{ans} from \texttt{nls()} or \texttt{minpack.lm::nlsLM} or our
new \texttt{nlsj} (interim package), then \texttt{ans\$m\$resid()} is
WEIGHTED. This needs clear documentation so users can get the answers
intended.

SHORT EXAMPLE??

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weed }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{5.308}\NormalTok{, }\FloatTok{7.24}\NormalTok{, }\FloatTok{9.638}\NormalTok{, }\FloatTok{12.866}\NormalTok{, }\FloatTok{17.069}\NormalTok{, }\FloatTok{23.192}\NormalTok{, }\FloatTok{31.443}\NormalTok{,}
          \FloatTok{38.558}\NormalTok{, }\FloatTok{50.156}\NormalTok{, }\FloatTok{62.948}\NormalTok{, }\FloatTok{75.995}\NormalTok{, }\FloatTok{91.972}\NormalTok{)}
\NormalTok{tt }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{12}
\NormalTok{weeddf }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(tt, weed)}
\NormalTok{wts }\OtherTok{\textless{}{-}} \FloatTok{0.5}\SpecialCharTok{\^{}}\NormalTok{tt }\CommentTok{\# simple weights}
\NormalTok{Asym}\OtherTok{\textless{}{-}}\DecValTok{1}\NormalTok{; xmid}\OtherTok{\textless{}{-}}\DecValTok{1}\NormalTok{; scal}\OtherTok{\textless{}{-}}\DecValTok{1}
\NormalTok{nowt}\OtherTok{\textless{}{-}}\FunctionTok{nls}\NormalTok{(weed }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SSlogis}\NormalTok{(tt, Asym, xmid, scal))}
\NormalTok{nowt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Nonlinear regression model
##   model: weed ~ SSlogis(tt, Asym, xmid, scal)
##    data: parent.frame()
##   Asym   xmid   scal 
## 196.19  12.42   3.19 
##  residual sum-of-squares: 2.59
## 
## Number of iterations to convergence: 0 
## Achieved convergence tolerance: 1.13e-06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nowt}\SpecialCharTok{$}\NormalTok{m}\SpecialCharTok{$}\FunctionTok{resid}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] -0.011900  0.032755 -0.092030 -0.208782 -0.392634  0.057594  1.105728
##  [8] -0.715786  0.107647  0.348396 -0.652592  0.287569
## attr(,"gradient")
##           Asym     xmid    scal
##  [1,] 0.027117  -1.6229  5.8103
##  [2,] 0.036737  -2.1769  7.1111
##  [3,] 0.049596  -2.8997  8.5628
##  [4,] 0.066645  -3.8266 10.1000
##  [5,] 0.089005  -4.9881 11.6015
##  [6,] 0.117921  -6.3988 12.8762
##  [7,] 0.154635  -8.0418 13.6607
##  [8,] 0.200186  -9.8498 13.6432
##  [9,] 0.255106 -11.6901 12.5267
## [10,] 0.319083 -13.3660 10.1313
## [11,] 0.390688 -14.6444  6.5083
## [12,] 0.467334 -15.3139  2.0038
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inpar}\OtherTok{\textless{}{-}}\FunctionTok{getInitial}\NormalTok{(weed }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SSlogis}\NormalTok{(tt, Asym, xmid, scal), }\AttributeTok{data=}\NormalTok{weeddf)}
\FunctionTok{print}\NormalTok{(inpar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Asym     xmid     scal 
## 196.1862  12.4173   3.1891
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlsr2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'nlsr2'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:nlsr':
## 
##     codeDeriv, findSubexprs, fnDeriv, isCALL, isMINUSONE, isONE,
##     isZERO, model2rjfun, model2ssgrfun, modelexpr, newDeriv,
##     newSimplification, nlfb, nlsDeriv, nlsSimplify, nlxb, resgr, resss,
##     rjfundoc, sysDerivs, sysSimplifications, wrapnlsr
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nowtx}\OtherTok{\textless{}{-}}\FunctionTok{nlxb}\NormalTok{(weed }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SSlogis}\NormalTok{(tt, Asym, xmid, scal), }\AttributeTok{start=}\FunctionTok{c}\NormalTok{(}\AttributeTok{Asym=}\DecValTok{1}\NormalTok{, }\AttributeTok{xmid=}\DecValTok{1}\NormalTok{, }\AttributeTok{scal=}\DecValTok{1}\NormalTok{), }\AttributeTok{control=}\FunctionTok{list}\NormalTok{(}\AttributeTok{japprox=}\StringTok{"jafwd"}\NormalTok{), }\AttributeTok{trace=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Using approximation  jafwd   is.character(ctrl$japprox)= TRUE 
## <<lamda: 1e-04  SS= 23527 ( NA ) at  Asym = 1  xmid = 1  scal = 1 f/j  1 / 0
## <<lamda: 4e-05  SS= 10887 ( 0.8264 ) at  Asym = 52.337  xmid = 48.234  scal = 180.33 f/j  2 / 1
## <<lamda: 0.016  SS= 8948.4 ( 0.41672 ) at  Asym = 80.893  xmid = 48.394  scal = 169.52 f/j  6 / 2
## <<lamda: 0.0064  SS= 8853.5 ( 0.068832 ) at  Asym = 89.895  xmid = 66.542  scal = 118.27 f/j  7 / 3
## <<lamda: 0.0256  SS= 8673.7 ( 0.080434 ) at  Asym = 103.26  xmid = 69.707  scal = 91.474 f/j  9 / 4
## <<lamda: 0.1024  SS= 8563.7 ( 0.059227 ) at  Asym = 110.79  xmid = 67.869  scal = 81.345 f/j  11 / 5
## <<lamda: 0.04096  SS= 8392.1 ( 0.12852 ) at  Asym = 129.64  xmid = 62.354  scal = 48.414 f/j  12 / 6
## <<lamda: 0.16384  SS= 7777.5 ( 0.17269 ) at  Asym = 139.58  xmid = 48.216  scal = 39.581 f/j  14 / 7
## <<lamda: 0.65536  SS= 7547.4 ( 0.11307 ) at  Asym = 142.31  xmid = 43.841  scal = 33.749 f/j  16 / 8
## <<lamda: 0.26214  SS= 6213 ( 0.22183 ) at  Asym = 149.97  xmid = 28.124  scal = 17.452 f/j  17 / 9
## <<lamda: 1.0486  SS= 2919.7 ( 0.33273 ) at  Asym = 152.81  xmid = 15.369  scal = 7.5443 f/j  19 / 10
## <<lamda: 4.1943  SS= 1903.2 ( 0.74156 ) at  Asym = 153.41  xmid = 9.8394  scal = 1.8083 f/j  21 / 11
## <<lamda: 1.6777  SS= 113.84 ( 0.73921 ) at  Asym = 153.46  xmid = 10.631  scal = 2.5864 f/j  22 / 12
## <<lamda: 0.67109  SS= 11.222 ( 0.66287 ) at  Asym = 153.64  xmid = 10.972  scal = 2.8505 f/j  23 / 13
## <<lamda: 0.26844  SS= 10.289 ( 0.19346 ) at  Asym = 154.03  xmid = 11.02  scal = 2.8798 f/j  24 / 14
## <<lamda: 0.10737  SS= 9.8337 ( 0.11332 ) at  Asym = 154.95  xmid = 11.055  scal = 2.889 f/j  25 / 15
## <<lamda: 0.04295  SS= 8.8717 ( 0.16811 ) at  Asym = 157.04  xmid = 11.133  scal = 2.9084 f/j  26 / 16
## <<lamda: 0.01718  SS= 7.2069 ( 0.21397 ) at  Asym = 161.34  xmid = 11.29  scal = 2.9469 f/j  27 / 17
## <<lamda: 0.0068719  SS= 5.1659 ( 0.30108 ) at  Asym = 168.61  xmid = 11.548  scal = 3.0079 f/j  28 / 18
## <<lamda: 0.0027488  SS= 3.5835 ( 0.33934 ) at  Asym = 178.1  xmid = 11.867  scal = 3.079 f/j  29 / 19
## <<lamda: 0.0010995  SS= 2.8237 ( 0.28331 ) at  Asym = 187.23  xmid = 12.155  scal = 3.1385 f/j  30 / 20
## <<lamda: 0.0004398  SS= 2.6123 ( 0.16163 ) at  Asym = 193.29  xmid = 12.334  scal = 3.1734 f/j  31 / 21
## <<lamda: 0.00017592  SS= 2.588 ( 0.056549 ) at  Asym = 195.68  xmid = 12.403  scal = 3.1863 f/j  32 / 22
## <<lamda: 7.0369e-05  SS= 2.5873 ( 0.010514 ) at  Asym = 196.14  xmid = 12.416  scal = 3.1889 f/j  33 / 23
## <<lamda: 2.8147e-05  SS= 2.5873 ( 0.0009007 ) at  Asym = 196.18  xmid = 12.417  scal = 3.1891 f/j  34 / 24
## <<lamda: 1.1259e-05  SS= 2.5873 ( 3.4226e-05 ) at  Asym = 196.19  xmid = 12.417  scal = 3.1891 f/j  35 / 25
## <<lamda: 4.5036e-06  SS= 2.5873 ( 6.5424e-07 ) at  Asym = 196.19  xmid = 12.417  scal = 3.1891 f/j  36 / 26
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nowtx}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## nlsr2  object: x 
## residual sumsquares =  2.5873  on  12 observations
##     after  27    Jacobian and  36 function evaluations
##   name            coeff          SE       tstat      pval      gradient    JSingval   
## Asym             196.186         11.31      17.35  3.167e-08  -5.714e-10       44.93  
## xmid             12.4173        0.3346      37.11  3.716e-11  -6.654e-08        15.6  
## scal             3.18908        0.0698      45.69  5.768e-12   1.567e-07      0.0474
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usewt }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(weed }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SSlogis}\NormalTok{(tt, Asym, xmid, scal), }\AttributeTok{weights=}\NormalTok{wts)}
\NormalTok{usewt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Nonlinear regression model
##   model: weed ~ SSlogis(tt, Asym, xmid, scal)
##    data: parent.frame()
##   Asym   xmid   scal 
## 199.86  12.50   3.19 
##  weighted residual sum-of-squares: 0.0196
## 
## Number of iterations to convergence: 2 
## Achieved convergence tolerance: 8.29e-06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usewt}\SpecialCharTok{$}\NormalTok{m}\SpecialCharTok{$}\FunctionTok{resid}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  0.0085640  0.0324442 -0.0176652 -0.0388479 -0.0579575  0.0163623
##  [7]  0.1042380 -0.0411766  0.0052509  0.0084324 -0.0194246 -0.0024053
## attr(,"gradient")
##           Asym     xmid    scal
##  [1,] 0.026498  -1.6157  5.8228
##  [2,] 0.035901  -2.1679  7.1333
##  [3,] 0.048474  -2.8890  8.6006
##  [4,] 0.065153  -3.8149 10.1617
##  [5,] 0.087046  -4.9775 11.6983
##  [6,] 0.115387  -6.3933 13.0222
##  [7,] 0.151425  -8.0483 13.8709
##  [8,] 0.196222  -9.8787 13.9297
##  [9,] 0.250362 -11.7553 12.8918
## [10,] 0.313611 -13.4827 10.5608
## [11,] 0.384641 -14.8251  6.9663
## [12,] 0.460954 -15.5631  2.4357
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# usewtx\textless{}{-}nlxb(weed \textasciitilde{} SSlogis(tt, Asym, xmid, scal), start=inpar, weights=wts,}
\CommentTok{\#              control=list(japprox="jafwd"), trace=TRUE)}
\CommentTok{\# usewtx}
\CommentTok{\# usewtx$resid}
\end{Highlighting}
\end{Shaded}

\hypertarget{interim-output-from-the-port-algorithm}{%
\subsubsection{Interim output from the ``port''
algorithm}\label{interim-output-from-the-port-algorithm}}

As the \texttt{nls()} \textbf{man} page states, when the ``port''
algorithm is used with the \texttt{trace} argument TRUE, the iterations
display the objective function value which is 1/2 the sum of squares (or
deviance). It is likely that the trace display is embedded in the
Fortran of the \texttt{nlminb} routine that is called to execute the
``port'' algorithm, but the discrepancy is nonetheless unfortunate for
users.

\hypertarget{failure-to-return-best-result-achieved}{%
\subsubsection{Failure to return best result
achieved}\label{failure-to-return-best-result-achieved}}

If \texttt{nls()} reaches a point where it cannot continue but has not
found a point where the relative offset convergence criterion is met, it
may simply exit, especially if a ``singular gradient'' (singular
Jacobian) is found. However, this may occur AFTER the function has made
considerable progress in reducing the sum of squared residuals. Here is
an abbreviated example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{time }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\DecValTok{1}\NormalTok{,  }\DecValTok{2}\NormalTok{,  }\DecValTok{3}\NormalTok{,  }\DecValTok{4}\NormalTok{,  }\DecValTok{6}\NormalTok{ , }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\NormalTok{conc }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\FloatTok{0.7}\NormalTok{, }\FloatTok{1.2}\NormalTok{, }\FloatTok{1.4}\NormalTok{, }\FloatTok{1.4}\NormalTok{, }\FloatTok{1.1}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.3}\NormalTok{)}
\NormalTok{NLSdata }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(time,conc)}
\NormalTok{NLSstart }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{lrc1 =} \SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\AttributeTok{lrc2 =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{A1 =} \DecValTok{150}\NormalTok{, }\AttributeTok{A2 =} \DecValTok{50}\NormalTok{) }\CommentTok{\# a starting vector (named!)}
\NormalTok{NLSformula }\OtherTok{\textless{}{-}}\NormalTok{ conc }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A1 }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{exp}\NormalTok{(lrc1) }\SpecialCharTok{*}\NormalTok{ time) }\SpecialCharTok{+}\NormalTok{ A2 }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{exp}\NormalTok{(lrc2) }\SpecialCharTok{*}\NormalTok{ time)}
\NormalTok{tryit }\OtherTok{\textless{}{-}} \FunctionTok{try}\NormalTok{(}\FunctionTok{nls}\NormalTok{(NLSformula, }\AttributeTok{data =}\NormalTok{ NLSdata, }\AttributeTok{start =}\NormalTok{ NLSstart, }\AttributeTok{trace =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 61216.    (3.56e+03): par = (-2 0.25 150 50)
## 2.1757    (2.23e+01): par = (-1.9991 0.31711 2.6182 -1.3668)
## 1.6211    (7.14e+00): par = (-1.9605 -2.6203 2.5753 -0.55599)
## Error in nls(NLSformula, data = NLSdata, start = NLSstart, trace = TRUE) : 
##   singular gradient
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(tryit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Error in nls(NLSformula, data = NLSdata, start = NLSstart, trace = TRUE) : \n  singular gradient\n"
## attr(,"class")
## [1] "try-error"
## attr(,"condition")
## <simpleError in nls(NLSformula, data = NLSdata, start = NLSstart, trace = TRUE): singular gradient>
\end{verbatim}

Note that the sum of squares has been reduced from 61216 to 1.6211, but
unless \texttt{trace} is invoked, the user will not get any information
about this. This almost trivial change to the \texttt{nls()} function
could be useful to R users.

\hypertarget{feature-partially-linear-models-and-their-specification}{%
\subsection{Feature: partially linear models and their
specification}\label{feature-partially-linear-models-and-their-specification}}

Specifying a model to a solver should, ideally, use the same syntax
across solver tools. Unfortunately, R allows multiple approaches within
different modelling tools, and within \texttt{nls()} itself.

One obvious case is that nonlinear modeling tools are a superset of
linear ones. Yet the explicit model
\texttt{y\ \textasciitilde{}\ a\ *\ x\ +\ b} does not work with the
linear modeling function \texttt{lm()}, which requires this model to be
specified as \texttt{y\ \textasciitilde{}\ x}.

Within \texttt{nls()}, consider the following FOUR different calling
sequences for the same problem, though an intuitive choice, labelled
fm2a, will not work. In this failed attempt, putting the \texttt{Asym}
parameter in the model causes the \texttt{plinear} algorithm to try to
add another term to the model. We believe this is unfortunate, and would
like to see a consistent syntax. At the time of writing, we do not
foresee a resolution for this issue. In the example, we have NOT
evaluated the commands to save space.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DNase1 }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(DNase, Run }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\CommentTok{\# select the data}
\DocumentationTok{\#\# using a selfStart model {-} do not specify the starting parameters}
\NormalTok{fm1 }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(density }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SSlogis}\NormalTok{(}\FunctionTok{log}\NormalTok{(conc), Asym, xmid, scal), DNase1)}
\FunctionTok{summary}\NormalTok{(fm1)}

\DocumentationTok{\#\# using conditional linearity {-} leave out the Asym parameter}
\NormalTok{fm2 }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(density }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{((xmid }\SpecialCharTok{{-}} \FunctionTok{log}\NormalTok{(conc)) }\SpecialCharTok{/}\NormalTok{ scal)),}
                 \AttributeTok{data =}\NormalTok{ DNase1, }\AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{xmid =} \DecValTok{0}\NormalTok{, }\AttributeTok{scal =} \DecValTok{1}\NormalTok{),}
                 \AttributeTok{algorithm =} \StringTok{"plinear"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fm2)}

\DocumentationTok{\#\# without conditional linearity}
\NormalTok{fm3 }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(density }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Asym }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{((xmid }\SpecialCharTok{{-}} \FunctionTok{log}\NormalTok{(conc)) }\SpecialCharTok{/}\NormalTok{ scal)),}
                 \AttributeTok{data =}\NormalTok{ DNase1,}
                 \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Asym =} \DecValTok{3}\NormalTok{, }\AttributeTok{xmid =} \DecValTok{0}\NormalTok{, }\AttributeTok{scal =} \DecValTok{1}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(fm3)}

\DocumentationTok{\#\# using Port\textquotesingle{}s nl2sol algorithm}
\NormalTok{fm4 }\OtherTok{\textless{}{-}} \FunctionTok{try}\NormalTok{(}\FunctionTok{nls}\NormalTok{(density }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Asym }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{((xmid }\SpecialCharTok{{-}} \FunctionTok{log}\NormalTok{(conc)) }\SpecialCharTok{/}\NormalTok{ scal)),}
                 \AttributeTok{data =}\NormalTok{ DNase1, }\AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Asym =} \DecValTok{3}\NormalTok{, }\AttributeTok{xmid =} \DecValTok{0}\NormalTok{, }\AttributeTok{scal =} \DecValTok{1}\NormalTok{),}
                 \AttributeTok{algorithm =} \StringTok{"port"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(fm4)}

\DocumentationTok{\#\# using conditional linearity AND Asym does NOT work}
\NormalTok{fm2a }\OtherTok{\textless{}{-}} \FunctionTok{try}\NormalTok{(}\FunctionTok{nls}\NormalTok{(density }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Asym }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{((xmid }\SpecialCharTok{{-}} \FunctionTok{log}\NormalTok{(conc)) }\SpecialCharTok{/}\NormalTok{ scal)), }
                 \AttributeTok{data =}\NormalTok{ DNase1, }\AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Asym=}\DecValTok{3}\NormalTok{, }\AttributeTok{xmid =} \DecValTok{0}\NormalTok{, }\AttributeTok{scal =} \DecValTok{1}\NormalTok{),}
                 \AttributeTok{algorithm =} \StringTok{"plinear"}\NormalTok{, }\AttributeTok{trace =} \ConstantTok{TRUE}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(fm2a)}
\end{Highlighting}
\end{Shaded}

\hypertarget{issue-code-structure-and-documentation-for-maintenance}{%
\subsection{Issue: code structure and documentation for
maintenance}\label{issue-code-structure-and-documentation-for-maintenance}}

The \texttt{nls()} code is structured in a way that inhibits both
maintenance and improvement. In particular, the iterative setup is such
that introduction of Marquardt stabilization is not easily available. To
obtain performance a lot of the code was in C with consequent calls and
returns that complicate the code. Over time, R has become much more
efficient on modern computers, and the need to use compiled C and
Fortran is less critical. Moreover, the burden for maintenance could be
reduced by moving code entirely to R.

While R and its packages are generally very well documented for usage,
that documentation rarely extends as far as it might in directions
helpful for code maintenance. The paucity of such documentation is
exacerbated by the mixed R/C/Fortran code base, where seemingly trivial
differences in things like representation of numbers or base index for
arrays lead to traps for unwary programmers.

For \texttt{nls()} the concern is demonstrated by a short email query
from John Nash to Doug Bates and his reply. This is NOT a criticism of
Prof.~Bates' (or any other) work, but a reflection on how difficult it
is to develop code in this subject area and to keep it maintainable. We
have experienced similar loss of understanding for some of our own
codes.

\begin{verbatim}
    ...
    https://gitlab.com/nashjc/improvenls/-/blob/master/Croucher-expandednlsnoc.R
    ... has the test problem and the expanded code. Around line 367 is where we are
    scratching our heads. The function code (from nlsModel()) is in the commented 
    lines below the call. This is

          # > setPars
          # function(newPars) {
          #   setPars(newPars)
          #   resid <<- .swts * (lhs - (rhs <<- getRHS())) # envir = thisEnv {2 x}
          #   dev   <<- sum(resid^2) # envir = thisEnv
          #   if(length(gr <- attr(rhs, "gradient")) == 1L) gr <- c(gr)
          #   QR <<- qr(.swts * gr) # envir = thisEnv
          #   (QR$rank < min(dim(QR$qr))) # to catch the singular gradient matrix
          # }

    I'm anticipating that we will be able to set up a (possibly inefficient) code
    with documentation that will be easier to follow and test, then gradually figure
    out how to make it more efficient.

    The equivalent from minpack.lm is

    setPars = function(newPars) {
                setPars(newPars)
                assign("resid", .swts * (lhs - assign("rhs", getRHS(),
                    envir = thisEnv)), envir = thisEnv)
                assign("dev", sum(resid^2), envir = thisEnv)
                assign("QR", qr(.swts * attr(rhs, "gradient")), envir = thisEnv)
                return(QR$rank < min(dim(QR$qr)))
            }

    In both there is the recursive call, which must have a purpose I don't understand.
\end{verbatim}

The reply was

\begin{verbatim}
    I'm afraid that I don't know the purpose of the recursive call either.  
    I know that I wrote the code to use a closure for the response, covariates, etc., 
    but I don't recall anything like a recursive call being necessary.
    ...
\end{verbatim}

\hypertarget{feature-indexed-parameters}{%
\subsection{Feature: indexed
parameters}\label{feature-indexed-parameters}}

The \textbf{man} file for \texttt{nls()} includes the following example
of a situation in which parameters are indexed. It also uses the
``plinear'' option as an added complication.

Running the example reveals that the answers for the parameters are NOT
indexed. We do not see \texttt{a{[}1{]},\ a{[}2{]},\ a{[}3{]}} but
\texttt{a1,\ a2,\ a3}. This is no doubt because programming for indexed
parameters is extremely challenging. However, we believe the current
structure is could cause confusion and error, and propose an alternative
approach below.

\hypertarget{feature-bounds-on-parameters}{%
\subsection{Feature: bounds on
parameters}\label{feature-bounds-on-parameters}}

There are many situations where the context of a problem constrains the
values of parameters. For example, one of us was asked many years ago to
estimate a model where one parameter estimate was negative. The client
complained ``But that is supposed to be the number of grain elevators in
Saskatchewan''. The number should have been a positive integer, and
likely less than a few thousand.

\texttt{nls()} can impose bounds on parameters, but only if the
\texttt{port} algorithm is used. Unfortunately, the manual states

\begin{quote}
The \texttt{algorithm\ =\ "port"} code appears unfinished, and does not
even check that the starting value is within the bounds. Use with
caution, especially where bounds are supplied.
\end{quote}

This is clearly unsatisfactory for general use, but does find bounded
solutions. Inclusion of bounds on parameters in nonlinear least squares
computations is relatively straightforward, and it is part of the
default method for package \texttt{nlsr}. Package \texttt{minpack.lm}
also includes provision for bounds, but the following script, which we
do not run here for reasons of space, shows that it does not find a good
solution for the scaled Hobbs test problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# bhobbsX.R \#\# bounded formula specification of problem using Hobbs Weed problem}
\NormalTok{weed }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{5.308}\NormalTok{, }\FloatTok{7.24}\NormalTok{, }\FloatTok{9.638}\NormalTok{, }\FloatTok{12.866}\NormalTok{, }\FloatTok{17.069}\NormalTok{, }\FloatTok{23.192}\NormalTok{, }\FloatTok{31.443}\NormalTok{,}
          \FloatTok{38.558}\NormalTok{, }\FloatTok{50.156}\NormalTok{, }\FloatTok{62.948}\NormalTok{, }\FloatTok{75.995}\NormalTok{, }\FloatTok{91.972}\NormalTok{)}
\NormalTok{tt }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{12}
\NormalTok{wf }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{y =}\NormalTok{ weed, }\AttributeTok{tt =}\NormalTok{ tt)}
\NormalTok{st }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{b1 =} \DecValTok{1}\NormalTok{, }\AttributeTok{b2 =} \DecValTok{1}\NormalTok{, }\AttributeTok{b3 =} \DecValTok{1}\NormalTok{) }\CommentTok{\# a default starting vector (named!)}
\NormalTok{wmods }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ b1 }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \DecValTok{10} \SpecialCharTok{*}\NormalTok{ b2 }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1} \SpecialCharTok{*}\NormalTok{ b3 }\SpecialCharTok{*}\NormalTok{ tt)) }\DocumentationTok{\#\# Scaled model}
\FunctionTok{require}\NormalTok{(minpack.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: minpack.lm
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{require}\NormalTok{(nlsr)}
\CommentTok{\# Hobbs scaled problem with bounds, formula specification}
\NormalTok{anlxb1 }\OtherTok{\textless{}{-}} \FunctionTok{nlxb}\NormalTok{(wmods, }\AttributeTok{start =}\NormalTok{ st, }\AttributeTok{data =}\NormalTok{ wf, }\AttributeTok{lower =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{upper =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Using nlsr::nlxb():"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Using nlsr::nlxb():
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(anlxb1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## nlsr2  object: anlxb1 
## residual sumsquares =  9.4726  on  12 observations
##     after  12    Jacobian and  12 function evaluations
##   name            coeff          SE       tstat      pval      gradient    JSingval   
## b1                     2U           NA         NA         NA           0       23.35  
## b2               4.43325            NA         NA         NA   -2.16e-07           0  
## b3                     3U           NA         NA         NA           0           0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# nlsLM seems NOT to work properly with bounds}
\NormalTok{anlsLM1b }\OtherTok{\textless{}{-}} \FunctionTok{nlsLM}\NormalTok{(wmods, }\AttributeTok{start =}\NormalTok{ st, }\AttributeTok{data =}\NormalTok{ wf, }\AttributeTok{lower =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{upper =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{); }\FunctionTok{cat}\NormalTok{(}\StringTok{"using minpack.lm::nlsLM():"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## using minpack.lm::nlsLM():
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(anlsLM1b)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Nonlinear regression model
##   model: y ~ 100 * b1/(1 + 10 * b2 * exp(-0.1 * b3 * tt))
##    data: wf
## b1 b2 b3 
##  2  6  3 
##  residual sum-of-squares: 881
## 
## Number of iterations to convergence: 2 
## Achieved convergence tolerance: 1.49e-08
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anlsb}\OtherTok{\textless{}{-}}\FunctionTok{nls}\NormalTok{(wmods, }\AttributeTok{start =}\NormalTok{ st, }\AttributeTok{data =}\NormalTok{ wf, }\AttributeTok{algorithm =} \StringTok{"port"}\NormalTok{, }\AttributeTok{lower =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{upper =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Using nls with \textquotesingle{}port\textquotesingle{}:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Using nls with 'port':
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(anlsb)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Nonlinear regression model
##   model: y ~ 100 * b1/(1 + 10 * b2 * exp(-0.1 * b3 * tt))
##    data: wf
##   b1   b2   b3 
## 2.00 4.43 3.00 
##  residual sum-of-squares: 9.47
## 
## Algorithm "port", convergence message: both X-convergence and relative convergence (5)
\end{verbatim}

\hypertarget{philosophical-considerations}{%
\subsubsection{Philosophical
considerations}\label{philosophical-considerations}}

Bounds on parameters raise some interesting and difficult questions
about how uncertainty in parameter estimates should be computed or
reported. That is, the traditional ``standard errors'' are generally
taken to imply symmetric intervals about the point estimate in which the
parameter may be expected to be found with some probability under
certain assumptions. Bounds change those assumptions. Hence,
\texttt{nlsr::nlxb()} does not compute standard errors nor their derived
statistics when bounds are active.

\hypertarget{goals-and-progress-of-our-effort-to-improve-nls}{%
\section{\texorpdfstring{Goals and progress of our effort to improve
\texttt{nls()}}{Goals and progress of our effort to improve nls()}}\label{goals-and-progress-of-our-effort-to-improve-nls}}

\hypertarget{code-rationalization-and-documentation}{%
\subsection{Code rationalization and
documentation}\label{code-rationalization-and-documentation}}

We want:

\begin{itemize}
\item
  to provide a packaged version of \texttt{nls()} (call it
  \texttt{nlsalt}) coded entirely in R that matches the features in base
  R or what is packaged in \texttt{nlspkg} as described in John C. Nash
  and Bhattacharjee (\protect\hyperlink{ref-PkgFromRbase22}{2022}).
\item
  to streamline the overall \texttt{nls()} infrastructure. By this we
  mean a re-factoring of the routines so they are better suited to
  maintenance of both the existing \texttt{nls()} methods and features
  as well as new features we or others would like to add.
\item
  to explain what we do, either in comments or separate maintainer
  documentation. Since we are complaining about the lack of explanatory
  material for the current code, we feel it is incumbent on us to
  provide such material for our own work, and if possible for the
  existing code.
\end{itemize}

These goals echo themes in the recent discussion Vidoni
(\protect\hyperlink{ref-Vidoni21}{2021}) and accompanying commentary.

\hypertarget{rationalization-of-formula-specifications}{%
\subsubsection{Rationalization of formula
specifications}\label{rationalization-of-formula-specifications}}

We have seen that \texttt{nls()} uses a different formula specification
from the default if the \texttt{plinear} algorithm is used. This is
unfortunate, since the user cannot then simply add
\texttt{algorithm="plinear"} to the call, making errors more likely.

While not consistent with the \texttt{lm()} specification of linear
models, nonlinear models need all parameters specified. That is, the
model formula should be written down as it would be computed. Thus a
linear model (which is valid input for nonlinear estimatin) should be
specified \texttt{y\ \textasciitilde{}\ a\ *\ x\ +\ b} rather than
\texttt{y\ \textasciitilde{}\ x}. Moreover, we want to avoid the
partially linear parameters appearing in the result object as
\texttt{.lin1}, \texttt{.lin2}, etc., where the actual position of
parameters in the model is not explicit.

A possible workaround to allow consistency would be to specify the
partially linear parameters when the \texttt{plinear} option is
specified. For example, we could use \texttt{algorithm="plinear(Asym)"}
while keeping the general model formula from the default specification.
This would allow for the output of different \texttt{algorithm} options
to be consistent. However, we have not yet tried to code such a change,
and welcome discussion from those working with other packages using
model formulas.

A further complication of model specification in R is that some formulas
require the user to surround a term with \texttt{I()} to inhibit
interpretation of arithmetic operators. We would prefer that formulas be
usable without this complication.

\hypertarget{rationalization-of-indexed-models}{%
\subsubsection{Rationalization of indexed
models}\label{rationalization-of-indexed-models}}

Indexed models clearly have a place in some areas of research. We need
to be able to process models such as
\texttt{Length\ \textasciitilde{}\ a{[}Strip{]}\ +\ b{[}Strip{]}\ *\ exp(-Conc\ /\ th)}
where \texttt{Strip} is the index. Data \texttt{Length} and
\texttt{Conc} are available for all observations and parameter
\texttt{th} applies to every fitted point, but \texttt{a{[}Strip{]}} and
\texttt{b{[}Strip{]}} depend on the index, which could be an
experimental run number, or some other categorization.

Given that \texttt{{[}\ {]}} are the R method to index arrays, their
presence in a model formula signals indexing. The question is how to
perform the correct calculations.

Our view is that inclusion of the indexing \textbf{within}
\texttt{nls()} introduces complexity that could be avoided by a suitable
wrapper function. The modeling formula must be expanded to encompass all
the index cases, likely with indexed parameters renamed e.g.,
\texttt{a{[}2{]}} becomes \texttt{a2} as at present with \texttt{nls()}.
However, we believe the wrapper should be able to process output to
present the indexed parameters correctly.

\hypertarget{streamlining-code}{%
\subsubsection{Streamlining code}\label{streamlining-code}}

We are pessimistic that the overall structure of \texttt{nls()} can be
streamlined due to the entanglement of so many features with the
complicated mix of R, C and Fortran. Indeed, despite digging into the
code over some months, we do not feel confident that we sufficiently
understand it nor that we could maintain it. On the other hand, we do
believe equivalent functionality can be built, but with at least some
differences in how the features will be accessed.

\hypertarget{tests-and-use-case-examples}{%
\subsection{Tests and use-case
examples}\label{tests-and-use-case-examples}}

Maintainers of packages need suitable tests and use-case examples in
order:

\begin{itemize}
\tightlist
\item
  to ensure packages work properly, in particular, giving results
  comparable to or better than the functions they are to replace;
\item
  to test individual solver functions to ensure they work across the
  range of calling mechanisms, that is, different ways of supplying
  inputs to the solver(s);
\item
  to pose ``silly'' inputs to see if these bad inputs are caught by the
  programs.
\end{itemize}

Such goals align with the aims of \textbf{unit testing} (e.g.,
\url{https://towardsdatascience.com/unit-testing-in-r-68ab9cc8d211},
Wickham (\protect\hyperlink{ref-HWtestthat11}{2011}), Wickham et al.
(\protect\hyperlink{ref-HWdevtools21}{2021}) and the conventional R
package testing tools).

\hypertarget{a-testing-package-nlscompare}{%
\subsubsection{A testing package:
NLSCompare}\label{a-testing-package-nlscompare}}

One of us has developed a working prototype package at
\url{https://github.com/ArkaB-DS/nlsCompare} . A primary design
objective of this is to allow the summarisation of multiple tests in a
compact output. The prototype has a vignette to illustrate its use.

\hypertarget{documentation-and-resources}{%
\subsection{Documentation and
resources}\label{documentation-and-resources}}

In our investigation, we built several resources, which are now part of
the repository \url{https://github.com/nashjc/RNonlinearLS/}. Particular
items are:

\begin{itemize}
\item
  A BibTex bibliography for use with all documents in this project, but
  which has wider application to nonlinear least squares projects in
  general
  (\url{https://github.com/nashjc/RNonlinearLS/blob/main/BibSupport/ImproveNLS.bib})
\item
  \texttt{MachID.R} offers a suggested concise summary function to
  identify a particular computational system used for tests. A
  discussion of how it was built and the resources needed across
  platforms is given in at
  \url{https://github.com/nashjc/RNonlinearLS/tree/main/MachineSummary}
\item
  John C. Nash and Bhattacharjee
  (\protect\hyperlink{ref-PkgFromRbase22}{2022}) is an explanation of
  the construction of the \texttt{nlspkg} from the \texttt{nls()} code
  in R-base.
\item
  As the 2021 Summer of Code period was ending, one of us (JN) was
  invited to prepare a review of optimization in R. Ideas from the
  present work have been instrumental in the creation of John C. Nash
  (\protect\hyperlink{ref-NashWires22}{2022}).
\end{itemize}

\hypertarget{strategic-choices-in-nonlinear-model-estimation}{%
\section{Strategic choices in nonlinear model
estimation}\label{strategic-choices-in-nonlinear-model-estimation}}

It is possible that \texttt{nls()} will remain more or less as it has
been for the past two decades. Given its importance to R, a focus of
discussion should be what needs fixing and how that should be
approached, and we welcome an opportunity to participate in such
conversations.

From a design point of view, a key difference in approach between
\texttt{nls()} and \texttt{nlsr::nlxb()} is that \texttt{nls()} builds a
large infrastructure, especially the \texttt{nlsModel()} function, from
which the Gauss-Newton iteration can be executed and other statistical
information such as profiles can be computed, while \texttt{nlxb()}
returns quite limited information, and computes what is needed on an
``as and when'' basis. This follows a path that one of us (JN)
established almost 50 years ago with the software that became John C.
Nash (\protect\hyperlink{ref-cnm79}{1979}), separating setup, solver,
and post-solution analysis phases of computations.

\hypertarget{opinions}{%
\subsection{Opinions}\label{opinions}}

To advance the stability and maintainability of R, we believe the
program objects (R functions) that are created by tools such as
\texttt{nls()} should have minimal interactions and side-effects. The
aspects of \texttt{nls()} that have given us the most trouble:

\begin{itemize}
\item
  The functions that compute the residuals and jacobians often presume
  the data and parameters needed are available in a particular
  environment. As long as the correct environment is used, this provides
  a short syntax to invoke the calculations. The danger is that the
  wrong data is accessed if the internal search finds a valid name that
  is not the object we want.
\item
  Weights, subsets, and various contextual controls such as that for the
  \texttt{na.action} option are similarly taken from the first available
  source. Again, this makes for a very simple invocation of
  calculations, but the context is hidden from the user. Furthermore, it
  can be difficult for those of us trying to maintain or improve the
  code to be certain we have the context correct.
\item
  The mixing of R, C and Fortran code was necessary for computational
  performance in the past. Lacking decent developer documentation,
  programmers now face a lot of work to fix or improve code. We believe
  in having at least a working reference version of code in a single
  programming language. If necessary, by measuring (``profiling'') the
  code, we can find bottlenecks and, if necessary, substitute just those
  slower parts of the reference code.
\item
  A design that isolates the setup, solution, and post-solution parts of
  for complicated calculations reduces the number of objects that must
  be kept in alignment.
\item
  All iterative codes should return the best solution they have found so
  far, even if there are untoward conditions reached, such as a singular
  Jacobian. This modification could be made to \texttt{nls()} in the
  short term.
\item
  Given the existence of examples of good practices such as analytic
  derivatives, stabilized solution of Gauss-Newton equations and bounds
  constrained parameters, base R tools should be moving to incorporate
  them.
\end{itemize}

\hypertarget{acknowledgement}{%
\section{Acknowledgement}\label{acknowledgement}}

Hans Werner Borchers has been helpful in developing the proposal for
this project and in comments on this and related work. Heather Turner
co-mentored the project and helped guide the progress of the work.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-bateswatts}{}}%
Bates, D. M., and D. G. Watts. 1988. \emph{Nonlinear Regression Analysis
and Its Applications}. Wiley.

\leavevmode\vadjust pre{\hypertarget{ref-BatesWatts81}{}}%
Bates, Douglas M., and Donald G. Watts. 1981. {``A Relative Offset
Orthogonality Convergence Criterion for Nonlinear Least Squares.''}
\emph{Technometrics} 23 (2): 179--83.

\leavevmode\vadjust pre{\hypertarget{ref-Huet2004}{}}%
Huet, S., A. Bouvier, M.-A. Poursat, and E. Jolivet. 2004.
\emph{Statistical Tools for Nonlinear Regression: A Practical Guide with
{S-PLUS} Examples, 2nd Edition}. Berlin \& New York: Springer-Verlag.

\leavevmode\vadjust pre{\hypertarget{ref-Levenberg1944}{}}%
Levenberg, Kenneth. 1944. {``A Method for the Solution of Certain
Non-Linear Problems in Least Squares.''} \emph{Quarterly of Applied
Mathematics} 2: 164-\/-168.

\leavevmode\vadjust pre{\hypertarget{ref-Marquardt1963}{}}%
Marquardt, Donald W. 1963. {``{An Algorithm for Least-Squares Estimation
of Nonlinear Parameters}.''} \emph{SIAM Journal on Applied Mathematics}
11 (2): 431--41.

\leavevmode\vadjust pre{\hypertarget{ref-MiguezNLRAA2021}{}}%
Miguez, Fernando. 2021. \emph{{nlraa: Nonlinear Regression for
Agricultural Applications}}.
\url{https://CRAN.R-project.org/package=nlraa}.

\leavevmode\vadjust pre{\hypertarget{ref-jn77ima}{}}%
Nash, John C. 1977. {``Minimizing a Nonlinear Sum of Squares Function on
a Small Computer.''} \emph{Journal of the Institute for Mathematics and
Its Applications} 19: 231--37.

\leavevmode\vadjust pre{\hypertarget{ref-cnm79}{}}%
---------. 1979. \emph{Compact Numerical Methods for Computers: Linear
Algebra and Function Minimisation}. Bristol: Adam Hilger.

\leavevmode\vadjust pre{\hypertarget{ref-NashWires22}{}}%
---------. 2022. {``Function Minimization and Nonlinear Least Squares in
r.''} \emph{WIREs Computational Statistics}, no. e1580.
https://doi.org/\url{https://doi.org/10.1002/wics.1580}.

\leavevmode\vadjust pre{\hypertarget{ref-PkgFromRbase22}{}}%
Nash, John C., and Arkajyoti Bhattacharjee. 2022. {``Making a Package
from Base {R} Files.''} \emph{R-Bloggers}, January.
\url{https://www.r-bloggers.com/2022/01/making-a-package-from-base-r-files/}.

\leavevmode\vadjust pre{\hypertarget{ref-nlsr-manual}{}}%
Nash, John C, and Duncan Murdoch. 2019. \emph{Nlsr: Functions for
Nonlinear Least Squares Solutions}.

\leavevmode\vadjust pre{\hypertarget{ref-nash1996nonlinear}{}}%
Nash, John C., and Paul Velleman. 1996. {``Nonlinear Estimation
Combining Visual Fitting with Optimization Methods.''} In
\emph{Proceedings of the Section on Physical and Engineering Sciences of
the American Statistical Association}, 256--61. American Statistical
Association.

\leavevmode\vadjust pre{\hypertarget{ref-Ratkowsky1983}{}}%
Ratkowsky, David A. 1983. \emph{Nonlinear Regression Modeling: A Unified
Practical Approach}. New York; Basel: Marcel Dekker Inc.

\leavevmode\vadjust pre{\hypertarget{ref-Vidoni21}{}}%
Vidoni, Melina. 2021. {``{Software Engineering and R Programming: A Call
for Research}.''} \emph{{The R Journal}} 13 (2): 6--14.
\url{https://doi.org/10.32614/RJ-2021-108}.

\leavevmode\vadjust pre{\hypertarget{ref-HWtestthat11}{}}%
Wickham, Hadley. 2011. {``Testthat: Get Started with Testing.''}
\emph{The R Journal} 3: 5--10.
\url{https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-HWdevtools21}{}}%
Wickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2021.
\emph{Devtools: Tools to Make Developing r Packages Easier}.
\url{https://CRAN.R-project.org/package=devtools}.

\end{CSLReferences}

\end{document}
